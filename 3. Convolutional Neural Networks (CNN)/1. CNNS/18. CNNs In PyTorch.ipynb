{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs In PyTorch\n",
    "\n",
    "To create a convolutional layer in PyTorch, you must first import the necessary module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a two part process to defining a convolutional layer and defining the feedforward behavior of a model (how an input moves through the layers of a network. First you must define a Model class and fill in two functions.\n",
    "\n",
    "## init\n",
    "\n",
    "You can define a convolutional layer in the __init__ function of by using the following format:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward\n",
    "\n",
    "Then, you refer to that layer in the forward function! Here, I am passing in an input image x and applying a ReLu function to the output of this layer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = F.relu(self.conv1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "\n",
    "- in_channels refers to the depth of an input. For a grayscale image, this depth = 1\n",
    "- out_channels refers to the desired depth of the output, or the number of filtered images you want to get as output\n",
    "- kernel_size is the size of your convolutional kernel (most commonly 3 for a 3x3 kernel)\n",
    "- stride and padding have default values, but should be set depending on how large you want your output to be in the spatial dimensions x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "Pooling layers take in a kernel_size and a stride. Typically the same value, the is the down-sampling factor. For example, the following code will down-sample an input's x-y dimensions, by a factor of 2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "self.pool = nn.MaxPool2d(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward\n",
    "\n",
    "Here, we see that poling layer being applied in the forward function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = F.relu(self.conv1(x))\n",
    "x = self.pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
