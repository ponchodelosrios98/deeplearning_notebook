{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "## ReLU Activation Function\n",
    "\n",
    "The purpose of an activation function is to scale the outputs of a layer so that they are a consistent, small value. Much like normalizing input values, this step ensures that our model trains efficiently!\n",
    "\n",
    "A ReLu activation function stands for \"rectified linear unit\" and is one of the most commonly used activation functions for hidden layers. It is an activation function, simply defined as the positive part of the input, x. So, for an input image with any negative pixel values, this would turn all those values to 0, black. You may hear this referred to as \"clipping\" the values to zero; meaning that is the lower bound.\n",
    "\n",
    "## Cross-Entropy Loss\n",
    "In the PyTorch documentation, you can see that the cross entropy loss function actually involves two steps:\n",
    "\n",
    "- It first applies a softmax function to any output is sees\n",
    "- Then applies NLLLoss; negative log likelihood loss\n",
    "\n",
    "Then it returns the average loss over a batch of data. Since it applies a softmax function, we do not have to specify that in the forward function of our model definition, but we could do this another way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
